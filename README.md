# ğŸ§  Agent IA Complet - Gemini + MCP + PocketFlow

Agent IA de production avec architecture modulaire inspirÃ©e de **PocketFlow**, **RRLA** et le protocole **MCP** (Model Context Protocol).

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚  HTML/CSS/JS â†’ Interface utilisateur moderne
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚  REST + SSE â†’ Endpoints API
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestrator (PocketFlow) â”‚  â†’ Coordonne le flow
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ Perception       â†’ Nettoie l'input
       â”œâ”€â†’ Interpretation   â†’ DÃ©tecte l'intention
       â”œâ”€â†’ Memory          â†’ Recherche contexte
       â”œâ”€â†’ Reasoning (RRLA)â†’ Raisonnement en 4 Ã©tapes
       â”œâ”€â†’ Synthesis       â†’ GÃ©nÃ¨re la rÃ©ponse
       â””â”€â†’ Action          â†’ Retourne le rÃ©sultat

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gemini LLM   â”‚  â†’ GÃ©nÃ©ration + Streaming
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server   â”‚  â†’ Outils extensibles
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ FonctionnalitÃ©s

### âœ… Core
- ğŸ§  **Architecture RRLA** : Raisonnement â†’ RÃ©flexion â†’ Logique â†’ Action
- ğŸ”„ **PocketFlow** : Pattern prep/exec/post avec shared contract
- ğŸ”Œ **MCP Protocol** : Outils extensibles via Model Context Protocol
- ğŸš€ **Streaming SSE** : GÃ©nÃ©ration token-par-token en temps rÃ©el
- ğŸ“Š **TraÃ§abilitÃ© complÃ¨te** : Debug trace de chaque exÃ©cution

### ğŸ› ï¸ Nodes ImplÃ©mentÃ©s
1. **Perception** : Nettoyage et normalisation des entrÃ©es
2. **InterprÃ©tation** : DÃ©tection d'intention et typage de tÃ¢che
3. **Memory** : Recherche contexte conversationnel
4. **Reasoning** : RRLA complet avec dÃ©composition en Ã©tapes
5. **Synthesis** : AgrÃ©gation et gÃ©nÃ©ration de rÃ©ponse
6. **Action** : Production de la rÃ©ponse finale

### ğŸ”§ Outils MCP
- `search_memory` : Recherche sÃ©mantique dans la mÃ©moire
- `store_memory` : Stockage d'Ã©lÃ©ments en mÃ©moire
- `analyze_sentiment` : Analyse de sentiment
- `extract_keywords` : Extraction de mots-clÃ©s
- `calculate` : Ã‰valuateur d'expressions mathÃ©matiques
- `get_current_context` : RÃ©cupÃ©ration du contexte agent

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis
- Python 3.11+
- ClÃ© API Gemini ([obtenir ici](https://ai.google.dev/))

### Installation

```bash
# 1. Cloner le repo
git clone <votre-repo>
cd agent-ia-gemini-mcp

# 2. CrÃ©er l'environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# 3. Installer les dÃ©pendances
pip install -e .

# Ou avec uv (plus rapide)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv
uv sync
```

### Configuration

```bash
# Copier le template .env
cp .env.example .env

# Ã‰diter .env et ajouter votre clÃ© Gemini
# GEMINI_API_KEY=your_key_here
```

### Lancer l'application

```bash
# Avec uvicorn directement
uvicorn backend.main:app --reload --port 8000

# Ou via Python
python -m backend.main

# Ou avec uv
uv run uvicorn backend.main:app --reload --port 8000
```

Ouvrir **http://localhost:8000** dans votre navigateur ğŸ‰

## ğŸ“š Documentation API

### Endpoints REST

```bash
# Chat classique (non-streaming)
POST /api/chat
{
  "input": "Explique-moi l'architecture RRLA",
  "user_id": "user123"
}

# Streaming SSE
GET /api/stream?prompt=Bonjour

# Informations MCP
GET /api/mcp/info
GET /api/mcp/tools
GET /api/mcp/tools/schema

# Appel d'outil MCP
POST /api/mcp/call
{
  "tool": "search_memory",
  "arguments": {"query": "test", "top_k": 5}
}

# Pipeline info
GET /api/pipeline/info

# Health check
GET /health

# Statistiques
GET /api/stats
```

## ğŸ§ª Tests

```bash
# Lancer tous les tests
pytest tests/ -v

# Avec couverture
pytest tests/ --cov=backend --cov-report=html

# Test spÃ©cifique
pytest tests/test_orchestrator.py -v
```

## ğŸ¨ Frontend

L'interface web inclut :
- **Chat conversationnel** avec historique
- **Mode streaming** via Server-Sent Events
- **Panneau debug** avec trace d'exÃ©cution
- **Indicateurs de confiance** pour chaque rÃ©ponse
- **Design moderne** dark theme

### Raccourcis clavier
- `Ctrl+K` : Effacer la conversation
- `Ctrl+D` : Toggle panneau debug
- `Ctrl+S` : Toggle mode streaming
- `Shift+Enter` : Toggle streaming (dans l'input)

## ğŸ—ï¸ Structure du Projet

```
agent-ia-gemini-mcp/
â”œâ”€ backend/
â”‚  â”œâ”€ core/                # PocketFlow core
â”‚  â”‚  â”œâ”€ shared.py         # Contract de donnÃ©es
â”‚  â”‚  â”œâ”€ base_node.py      # Interface Node
â”‚  â”‚  â””â”€ orchestrator.py   # Orchestrateur central
â”‚  â”œâ”€ nodes/               # Modules fonctionnels
â”‚  â”‚  â”œâ”€ perception.py
â”‚  â”‚  â”œâ”€ interpretation.py
â”‚  â”‚  â”œâ”€ reasoning.py      # RRLA
â”‚  â”‚  â”œâ”€ synthesis.py
â”‚  â”‚  â”œâ”€ action.py
â”‚  â”‚  â””â”€ memory.py
â”‚  â”œâ”€ llm/                 # Client Gemini
â”‚  â”‚  â”œâ”€ gemini_client.py
â”‚  â”‚  â”œâ”€ prompt_builder.py
â”‚  â”‚  â””â”€ token_counter.py
â”‚  â”œâ”€ mcp/                 # MCP Server
â”‚  â”‚  â”œâ”€ server.py
â”‚  â”‚  â”œâ”€ tools.py
â”‚  â”‚  â””â”€ schemas.py
â”‚  â”œâ”€ memory/              # SystÃ¨me mÃ©moire
â”‚  â”‚  â”œâ”€ vector_store.py
â”‚  â”‚  â”œâ”€ graph_memory.py
â”‚  â”‚  â””â”€ context_manager.py
â”‚  â”œâ”€ utils/               # Utilitaires
â”‚  â”‚  â”œâ”€ logger.py
â”‚  â”‚  â””â”€ validators.py
â”‚  â”œâ”€ main.py              # FastAPI app
â”‚  â””â”€ config.py            # Configuration
â”œâ”€ frontend/               # Interface web
â”‚  â”œâ”€ index.html
â”‚  â”œâ”€ styles.css
â”‚  â””â”€ app.js
â”œâ”€ tests/                  # Tests unitaires
â”œâ”€ pyproject.toml          # DÃ©pendances
â””â”€ README.md
```

## ğŸ”§ Configuration AvancÃ©e

### Variables d'environnement

```bash
# API
GEMINI_API_KEY=your_key
GEMINI_MODEL=gemini-1.5-flash-latest  # ou gemini-1.5-pro

# Server
HOST=0.0.0.0
PORT=8000
DEBUG=true

# Memory
VECTOR_STORE_DIM=768
MAX_CONTEXT_LENGTH=8000

# MCP
MCP_SERVER_NAME=agent-ia-mcp
MCP_VERSION=1.0.0
```

## ğŸ“ Architecture RRLA

Le systÃ¨me de raisonnement RRLA se dÃ©compose en 4 Ã©tapes :

### 1. Raisonnement (R1)
DÃ©compose le problÃ¨me en Ã©tapes logiques
```
"CrÃ©er un site web" â†’ [
  1. Choisir la stack technique
  2. CrÃ©er la structure HTML
  3. Styliser avec CSS
  4. Ajouter l'interactivitÃ© JS
]
```

### 2. RÃ©flexion (R2)
Ã‰value chaque Ã©tape (faisabilitÃ©, prioritÃ©, risques)
```
Ã‰tape 1: feasibility=0.9, priority=5, risks=[]
Ã‰tape 2: feasibility=0.8, priority=4, risks=["complexitÃ©"]
```

### 3. Logique (L)
Construit la chaÃ®ne d'exÃ©cution
```
sequence: [1, 2, 3, 4]
critical_path: [1, 2]
dependencies: {3: [2], 4: [2, 3]}
```

### 4. Action (A)
DÃ©cide et exÃ©cute
```
action_type: "generate_response"
confidence: 0.85
requires_tools: false
```

## ğŸ”Œ Ã‰tendre avec MCP

### Ajouter un nouvel outil

```python
# 1. CrÃ©er la fonction handler
async def mon_outil(param1: str, param2: int) -> dict:
    # Logique de l'outil
    return {"result": "ok"}

# 2. DÃ©finir le schÃ©ma
MON_OUTIL_SCHEMA = {
    "type": "object",
    "properties": {
        "param1": {"type": "string"},
        "param2": {"type": "integer"}
    },
    "required": ["param1"]
}

# 3. Enregistrer l'outil
from backend.mcp import mcp_server, MCPTool

mcp_server.register_tool(MCPTool(
    name="mon_outil",
    description="Description de l'outil",
    input_schema=MON_OUTIL_SCHEMA,
    handler=mon_outil
))
```

## ğŸ“Š Performance

| OpÃ©ration | Temps moyen |
|-----------|-------------|
| Perception | ~5ms |
| InterprÃ©tation | ~10ms |
| Raisonnement (simple) | ~50ms |
| Raisonnement (RRLA) | ~500ms |
| SynthÃ¨se | ~800ms |
| **Total (simple)** | **~100ms** |
| **Total (RRLA)** | **~1.4s** |

*Tests sur machine moyenne avec Gemini 1.5 Flash*

## ğŸ› Debugging

### Activer les logs dÃ©taillÃ©s

```python
from backend.utils.logger import setup_logger
setup_logger(log_level="DEBUG", log_file="logs/agent.log")
```

### Inspecter le flow

```python
# Via l'API
result = await orchestrator.run(shared)
trace = shared.get_trace()

for entry in trace:
    print(f"{entry.node}: {entry.status} ({entry.duration_ms}ms)")
```

## ğŸ¤ Contribution

Les contributions sont bienvenues !

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/ma-feature`)
3. Commit (`git commit -m 'Ajoute ma feature'`)
4. Push (`git push origin feature/ma-feature`)
5. Ouvrir une Pull Request

### Style de code
- Black pour le formatage
- Ruff pour le linting
- Type hints partout
- Docstrings pour les fonctions publiques

## ğŸ“ Roadmap

- [ ] IntÃ©gration vector store (FAISS)
- [ ] SystÃ¨me de mÃ©moire long terme
- [ ] Support multi-modÃ¨les (OpenAI, Anthropic)
- [ ] Authentification utilisateurs
- [ ] Mode multi-agents
- [ ] Plugins dynamiques
- [ ] Interface de configuration web
- [ ] MÃ©triques et analytics

## ğŸ“„ Licence

MIT License - voir [LICENSE](LICENSE)

## ğŸ™ Remerciements

- [Google Gemini](https://ai.google.dev/) pour le LLM
- [FastAPI](https://fastapi.tiangolo.com/) pour le framework web
- [PocketFlow](https://github.com/hamada-ai/pocket-flow) pour l'inspiration architecture
- [MCP](https://www.anthropic.com/news/model-context-protocol) pour le protocole d'outils

## ğŸ“§ Contact

Pour toute question : [ouvrir une issue](https://github.com/votre-repo/issues)

---

**Fait avec â¤ï¸ et â˜• par [Votre Nom]**
