# ðŸ§  Agent IA Complet - Gemini + MCP + PocketFlow

Agent IA de production avec architecture modulaire inspirÃ©e de PocketFlow, RRLA et le protocole MCP.

## ðŸ—ï¸ Architecture

- Backend: FastAPI + Python 3.11+
- LLM: Google Gemini (streaming + function calling)
- Orchestration: PocketFlow (Nodes + Shared contract)
- Protocole: MCP (Model Context Protocol)
- Frontend: HTML/JS/CSS vanilla (pas de Node requis)

```
agent-ia-gemini-mcp/
â”œâ”€ pyproject.toml
â”œâ”€ .env.example
â”œâ”€ README.md
â”œâ”€ backend/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ core/
â”‚  â”‚  â”œâ”€ shared.py
â”‚  â”‚  â”œâ”€ orchestrator.py
â”‚  â”‚  â””â”€ base_node.py
â”‚  â”œâ”€ nodes/
â”‚  â”‚  â”œâ”€ perception.py
â”‚  â”‚  â”œâ”€ interpretation.py
â”‚  â”‚  â”œâ”€ reasoning.py
â”‚  â”‚  â”œâ”€ synthesis.py
â”‚  â”‚  â”œâ”€ action.py
â”‚  â”‚  â””â”€ feedback.py
â”‚  â”œâ”€ llm/
â”‚  â”‚  â”œâ”€ gemini_client.py
â”‚  â”‚  â”œâ”€ prompt_builder.py
â”‚  â”‚  â””â”€ token_counter.py
â”‚  â”œâ”€ mcp/
â”‚  â”‚  â”œâ”€ server.py
â”‚  â”‚  â”œâ”€ tools.py
â”‚  â”‚  â””â”€ schemas.py
â”‚  â”œâ”€ memory/
â”‚  â”‚  â”œâ”€ vector_store.py
â”‚  â”‚  â”œâ”€ graph_memory.py
â”‚  â”‚  â””â”€ context_manager.py
â”‚  â””â”€ utils/
â”‚     â”œâ”€ logger.py
â”‚     â””â”€ validators.py
â”œâ”€ frontend/
â”‚  â”œâ”€ index.html
â”‚  â”œâ”€ app.js
â”‚  â””â”€ styles.css
â””â”€ tests/
   â”œâ”€ test_orchestrator.py
   â”œâ”€ test_nodes.py
   â””â”€ test_mcp.py
```

## ðŸš€ DÃ©marrage Rapide

1) Installation

```bash
# Installer uv (gestionnaire Python ultra-rapide)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Aller dans le dossier
cd agent-ia-gemini-mcp

# CrÃ©er l'environnement virtuel
uv venv

# Installer les dÃ©pendances
uv sync
```

2) Configuration

```bash
# Copier le template .env
cp .env.example .env
# Ouvrir .env et dÃ©finir votre clÃ©
# GEMINI_API_KEY=your_key_here
```

3) Lancer

```bash
# Activer l'environnement
source .venv/bin/activate        # Linux/Mac
# .venv\\Scripts\\activate       # Windows

# DÃ©marrer l'API (auto-reload)
uv run uvicorn backend.main:app --reload --port 8000
```

Ouvrir http://localhost:8000

## ðŸ“‹ FonctionnalitÃ©s

- Perception: Nettoyage et normalisation des entrÃ©es
- InterprÃ©tation: DÃ©tection d'intention et typage de tÃ¢che
- Raisonnement (RRLA): DÃ©composition, rÃ©flexion, logique, action
- SynthÃ¨se: AgrÃ©gation des rÃ©sultats
- Action: Production de la rÃ©ponse finale
- SSE Streaming: Token-par-token en temps rÃ©el
- MCP Server: Outils exposÃ©s via protocole
- TraÃ§abilitÃ©: Trace complÃ¨te de chaque exÃ©cution

## ðŸŽ¯ Endpoints API

- GET  /                    â€“ Interface web
- POST /api/chat            â€“ Chat REST (non-streaming)
- GET  /api/stream          â€“ Chat SSE (streaming)
- GET  /api/mcp/info        â€“ Info serveur MCP
- GET  /api/mcp/tools       â€“ Liste des outils MCP
- POST /api/mcp/call        â€“ Appel d'un outil MCP
- GET  /health              â€“ Health check

## ðŸ§ª Tests

```bash
uv run pytest tests/ -v --cov=backend
```

Les tests mockent l'API Gemini pour Ã©viter tout appel rÃ©seau.

## ðŸ“š Notes d'Architecture

- RRLA
  - R â€“ Raisonnement: DÃ©composition du problÃ¨me
  - R â€“ RÃ©flexion: Ã‰valuation des options
  - L â€“ Logique: ChaÃ®nage d'infÃ©rences
  - A â€“ Action: Prise de dÃ©cision
- MCP
  - Outils exposÃ©s via `backend/mcp/server.py`
  - Exemple: `search_memory`

### Shared Contract (PocketFlow)

Toutes les donnÃ©es transitent par le `Shared` store:

```python
shared.set_context("user_input", text)
shared.set_result("reasoning", decision)
shared.add_trace(entry)
```

## ðŸ› ï¸ DÃ©veloppement

Ajouter un Node

```python
# backend/nodes/mon_node.py
from backend.core.base_node import BaseNode

class MonNode(BaseNode):
    def __init__(self):
        super().__init__("mon_node")
    async def exec(self, input_data):
        return {"result": "..."}
```

Enregistrer dans l'orchestrateur si nÃ©cessaire.

Ajouter un Outil MCP

```python
from backend.mcp.server import mcp_server, MCPTool

async def mon_outil(param: str) -> dict:
    return {"resultat": f"TraitÃ©: {param}"}

mcp_server.register_tool(MCPTool(
    name="mon_outil",
    description="Description de l'outil",
    input_schema={
        "type": "object",
        "properties": {"param": {"type": "string"}}
    },
    handler=mon_outil
))
```

## âœ… Production-Ready

- Retry logic et streaming pour Gemini
- Orchestrateur modulaire
- Lazy-init du client LLM (tests sans env vars)
- Logging structurÃ©
- Tests unitaires avec mocks

## ðŸ“„ Licence

MIT
